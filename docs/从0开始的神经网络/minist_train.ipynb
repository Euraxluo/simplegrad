{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 写一个手写字体识别的两层网络\n",
    "输入层为tanh,输出层为softmax,无隐藏层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 获取数据集\n",
    "import math\n",
    "\n",
    "\n",
    "def fetch_mnist():\n",
    "    def fetch(url):\n",
    "        import requests, gzip, os, hashlib, numpy\n",
    "        fp = os.path.join(\"/tmp\", hashlib.md5(url.encode('utf-8')).hexdigest())\n",
    "        if os.path.isfile(fp):\n",
    "            with open(fp, \"rb\") as f:\n",
    "                dat = f.read()\n",
    "        else:\n",
    "            with open(fp, \"wb\") as f:\n",
    "                dat = requests.get(url).content\n",
    "                f.write(dat)\n",
    "        return numpy.frombuffer(gzip.decompress(dat), dtype=numpy.uint8).copy()\n",
    "\n",
    "    X_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "    Y_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "    X_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "    Y_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "    return X_train, Y_train, X_test, Y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = fetch_mnist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "for i in range(10):\n",
    "    s = X_train[i]\n",
    "    s_dgit = s.reshape(28, 28)\n",
    "    plt.imshow(s_dgit, cmap=mpl.cm.binary)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28 * 28)\n",
    "X_test = X_test.reshape(-1, 28 * 28)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.shape  # (60000, 28, 28) 6000个样本,28*28"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train.shape  # 6000 个样本的label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 激活函数tanh,一般用于输入层或者隐藏层\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1 / (np.cosh(x)) ** 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def d_relu(x):\n",
    "    return -np.minimum(0, x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 一般用于输出层,主要用于多分类,因为其函数输出和为1,符合概率论,一般概率最高的就是最可能的类别\n",
    "def softmax(x):\n",
    "    exp = np.exp(x - x.max())\n",
    "    return exp / exp.sum()\n",
    "\n",
    "\n",
    "def d_softmax(x):\n",
    "    sm = softmax(x)\n",
    "    return np.diag(sm) - np.outer(sm, sm)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "假设我们构造一个两层的神经网络:\n",
    "设A为tanh,则输入层线性代数公式为\n",
    "$layer_{input} = A(X+b_0)$\n",
    "设B为softmax,则\n",
    "$layer_output = B(layer_{input}*W_1+b_1)$\n",
    "\n",
    "总的来说是希望在经过多层神经网络的线性或者非线性变换后得到一个output,其中有每一层的变换函数的权重和误差.最后将这个output,基于梯度下降和反向传播,得到确切的权重和误差值."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_output_dimensions = [X_train[0].shape, (10,)]  #输入输出的维度\n",
    "each_layer_activation_function = [tanh, softmax]  #我们只有输入层和输出层\n",
    "activation_differential_function = {tanh: d_tanh, softmax: d_softmax, relu: d_relu}  #我们只有输入层和输出层\n",
    "\n",
    "# 参数初始化\n",
    "parameter_placeholder = [\n",
    "    {'b': [0, 0]},\n",
    "    {'b': [0, 0], 'w': [-math.sqrt(6 / (sum(input_output_dimensions[0]) + sum(input_output_dimensions[1]))),\n",
    "                        math.sqrt(6 / (sum(input_output_dimensions[0]) + sum(input_output_dimensions[1])))]\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# layer初始化函数\n",
    "def layer_param_uniform_init(*x, min=-1, max=1):\n",
    "    ret = np.random.uniform(min, max, size=x) / np.sqrt(np.prod(x))\n",
    "    return ret.astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 初始化layer层的参数\n",
    "def init_parameter_b(layer):\n",
    "    dist = parameter_placeholder[layer]['b']  #初始化b参数,b:input_output_dimensions[layer]\n",
    "    # return layer_param_uniform_init(*input_output_dimensions[layer], min=dist[0], max=dist[1])\n",
    "    return np.random.rand(np.prod(input_output_dimensions[layer])) * (dist[1] - dist[0]) + dist[0]\n",
    "\n",
    "\n",
    "def init_parameter_w(layer):\n",
    "    dist = parameter_placeholder[layer]['w']  #初始化w参数,w:input_output_dimensions[layer - 1], input_output_dimensions[layer]\n",
    "    # return layer_param_uniform_init(*input_output_dimensions[layer - 1], *input_output_dimensions[layer], min=dist[0], max=dist[1])\n",
    "    return np.random.rand(np.prod(input_output_dimensions[layer - 1]), np.prod(input_output_dimensions[layer])) * (dist[1] - dist[0]) + dist[0]\n",
    "\n",
    "\n",
    "def init_all_params() -> list:\n",
    "    layers = []\n",
    "    for layer in range(len(parameter_placeholder)):\n",
    "        layer_parameter = {}\n",
    "        for param in parameter_placeholder[layer].keys():\n",
    "            if param == 'b':\n",
    "                layer_parameter['b'] = init_parameter_b(layer)\n",
    "                continue\n",
    "            if param == 'w':\n",
    "                layer_parameter['w'] = init_parameter_w(layer)\n",
    "                continue\n",
    "        layers.append(layer_parameter)\n",
    "    return layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forward(img, params):\n",
    "    \"\"\"\n",
    "    正向传播\n",
    "    :param img:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    l_0_in = img + params[0]['b']  #第一层的输入就是图片和参数\n",
    "    l_0_out = each_layer_activation_function[0](l_0_in)\n",
    "    l_1_in = np.dot(l_0_out, params[1]['w']) + params[1]['b']  #第二层的输入是第一层的输出，参数，以及一个新的激活函数\n",
    "    l_1_out = each_layer_activation_function[1](l_1_in)  #第二层的输出刚好是output\n",
    "    return l_1_out\n",
    "\n",
    "\n",
    "params = init_all_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 设置one_hot编码,同事损失函数\n",
    "one_hat = np.identity(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 损失函数 mse\n",
    "def square_loss(img, label, params):\n",
    "    y_pred = forward(img, params)\n",
    "    y = one_hat[label]\n",
    "    return np.dot(y - y_pred, y - y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grad_params(img, label, params):\n",
    "    \"\"\"\n",
    "    梯度计算\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    l_0_in = img + params[0]['b']  #第一层的输入就是图片和参数\n",
    "    l_0_out = each_layer_activation_function[0](l_0_in)\n",
    "    l_1_in = np.dot(l_0_out, params[1]['w']) + params[1]['b']  #第二层的输入是第一层的输出，参数，以及一个新的激活函数\n",
    "    l_1_out = each_layer_activation_function[1](l_1_in)  #第二层的输出刚好是output\n",
    "\n",
    "    diff = one_hat[label] - l_1_out\n",
    "    act_1 = activation_differential_function[each_layer_activation_function[0]](l_0_in)\n",
    "    act_2 = activation_differential_function[each_layer_activation_function[1]](l_1_in)\n",
    "    act_2 = np.dot(act_2, diff)\n",
    "\n",
    "    grad_b1 = -2 * act_2\n",
    "    grad_w1 = -2 * np.outer(l_0_in, act_2)\n",
    "    grad_b0 = -2 * act_1 * np.dot(params[1]['w'], act_2)\n",
    "    return {'w1': grad_w1, 'b1': grad_b1, 'b0': grad_b0}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_batch(current_batch, params):\n",
    "    grad_accu = grad_params(X_train[current_batch * batch_size], Y_train[current_batch * batch_size], params)\n",
    "    for i in range(1, batch_size):\n",
    "        temp = grad_params(X_train[current_batch * batch_size + i], Y_train[current_batch * batch_size + i], params)\n",
    "        for k in grad_accu.keys():\n",
    "            grad_accu[k] += temp[k]\n",
    "    for k in grad_accu.keys():\n",
    "        grad_accu[k] = grad_accu[k] / batch_size\n",
    "    return grad_accu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def combine(params, grad, learn_rate=1):\n",
    "    tmp = copy.deepcopy(params)\n",
    "    tmp[0]['b'] -= grad['b0'] * learn_rate\n",
    "    tmp[1]['b'] -= grad['b1'] * learn_rate\n",
    "    tmp[1]['w'] -= grad['w1'] * learn_rate\n",
    "    return tmp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def valid_loss(params):\n",
    "    loss = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        loss += square_loss(X_test[i], label=Y_test[i], params=params)\n",
    "    return loss / X_test.shape[0]\n",
    "\n",
    "\n",
    "def valid_accu(params):\n",
    "    correct = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        correct.append(forward(X_test[i], params).argmax() == Y_test[i])\n",
    "    return correct.count(True) / len(correct)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 训练\n",
    "batch_size = 100\n",
    "epoch = 5\n",
    "train_datas = X_train.shape[0]\n",
    "for a in range(epoch):\n",
    "    for i in range(int(train_datas / batch_size)):\n",
    "        grad_temp = train_batch(i, params)\n",
    "        params = combine(params, grad_temp)\n",
    "    print(f\"epoch:{a},valid_accu:{valid_accu(params)},valid_loss:{valid_loss(params)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}